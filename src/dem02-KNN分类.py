'''
KNN : K近邻算法
    实现思路：
1.分类问题
    适用于：有特征，有标签，且标签是不连续的（离散的）
2.回归问题.
    适用于：有特征，有标签，且标签是连续的.
  
KNN算法，分类问题思路如下：
    1.计算测试集和每个训练的样本之间的距离.
    2.基于距离进行升序排列.
    3.找到最近的K个样本.
    4.K个样本进行投票.
    5.票数多的结果，作为最终的预测结果。  

代码实现思路：
    1.导包.
    2.准备数据集（测试集和训练集）
    3.创建（KNN分类模型）模型对象.
    4.模型训练
    5.模型预测.
    # 6.模型评估.
'''
# 1.导包.
from sklearn.neighbors import KNeighborsClassifier

# 2.准备数据集（测试集和训练集）
#训练集的特征数据 因为特征可以有多个特征，所以是一个二维数组
X_train = [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9]] 
#训练集的标签数据 标签是离散的,所以是一个一维数组
y_train = [0,0,1,1,2,2,3,3,4,4]    
X_test=[[100]]   #测试集的特征数据 
# y_test=[4]     #测试集的标签数据

# 3.创建（KNN分类模型）模型对象.
#estimator：估计器，模型对象，也可以用变量名model做接收。
estimator1 = KNeighborsClassifier(n_neighbors=2)

# 4.模型训练
#传入：训练集的特征数据，训练集的标签数据
estimator1.fit(X_train, y_train)

# 5.模型预测.
#传入：测试集的特征数据,预测结果保存在y_pre中
y_pre = estimator1.predict(X_test)

print("预测结果：", y_pre)

# 6.模型评估.

