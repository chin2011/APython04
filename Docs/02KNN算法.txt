
==============KNN算法（K近邻算法）============
	欧式距离：
		对应维度差值
		平方和，开平方根
	切比雪夫距离,	曼哈顿距离,	城市街区距离,	闵式距离

KNN:	K近邻	K个临近的邻居


--------------------KNN算法简介----------------------
	KNN思想、分类和回归问题处理流程
	
	2. 知道K近邻算法
		分类流程:-----投票
		回归流程:-----均值
	
K-近邻算法（K Nearest Neighbor，简称KNN）。
	比如：根据你的“邻居”来推断出你的类别
KNN算法思想：		<---距离
	如果一个样本在特征空间中的k个最相似的样本中的大多数属于某一个类别，则该样本也属于这个类别

→样本相似性：
	样本都是属于一个任务数据集的。样本距离--越近则越相似。
→欧式距离= 对应维度差值平方和，开平方根
	二维平面上点a（x1,y1）与b（x2.y2）间的欧氏距离：
			d2=√(x-x)²+(y-y2)²		
	三维:	d2=√(x-x2)²+(y-y2)²+(z-z2)²

←K值过小：用较小邻域中的训练实例进行预测
	容易受到异常点的影响
	K值的减小就意味着整体模型变得复杂，容易发生--过拟合

←K值过大：用较大邻域中的训练实例进行预测
	受到样本均衡的问题
	且K值的增大就意味着整体的模型变得简单，欠拟合

分类流	--标签不连续--投票
	1.计算未知样本到每一个训练样本的距离
	2.将训练样本根据距离大小升序排列
	3.取出距离最近的 K个训练样本
→	4.进行多数表决，统计 K个样本中哪个类别的样本个数最多
	5.将未知的样本归属到出现次数最多的类别

回归流程--标签连续---均值	
	1.计算未知样本到每一个训练样本的距离
	2.将训练样本根据距离大小升序排列
	3.取出距离最近的 K个训练样本
→	4.把这个K个样本的目标值计算其平均值
	5.作为将未知的样本预测的值


--------------------KNN算法API介绍--------------------
	分类、回归实现  <----物以类聚
	
KNN分类API
	sklearn. neighbors. KNeighborsClassifier (n_neighbors=5)
	n_neighbors：int,可选（默认=5），k_neighbors查询默认使用的邻居数

→unbiased estimator[数] 无偏估计量 ; 不偏点估计式 ; 不偏估计数 ; [数] 无偏估计值
→Sine estimator 正弦估计量


--------------------距离度量distance measure--------------------
常用距离计算方法
	
1．欧氏距离 Euclidean Distance
	直观的距离度量方法，
	两个点在空间中的距离一般都是指欧氏距离 
		d2=√(x-x)²+(y-y2)²

2．曼哈顿距离（ManhattanDistance)
	也称为“城市街区距离”（City Block distance），曼哈顿城市特点：横平竖直
	二维平面两点a（x1,y1）与b（x2.y2）间的曼哈顿距离
		d2=|x1-x2| + |y1-y2|
		
	：对应维度差值的绝对值，求和
	
3．切比雪夫距离ChebyshevDistance
	二维平面两点a（x1,y1）与b（x2.y2）间的切比雪夫距离
		d12=max( |x1-x2| , |y1-y2| )		不是 +

	国际象棋中，国王可以直行、横行、斜行，所以国王走一步可以移动到相邻8个方格中的任意一个。国王从格子（x1，y1）走到格子（x2，y2）最少需要多少步？这个距离就叫切比雪夫距离。

4.   闵可夫斯基距离MinkowskiDistance
	不是一种新的距离的度量方式。 是对多个距离度量公式的概括性的表述
	
其中p是一个变参数：
	当 p=1 时，就是曼哈顿距离；
	当 p=2 时，就是欧氏距离；
	当 p→∞o 时，就是切比雪夫距离
根据 p的不同，闵氏距离可表示某一类种的距离
σ

--------------------特征预处理--------------------
	归一化、标准化、鸢尾花识别案例
	
归一化：通过对原始数据进行变换把数据映射到【mi,mx】（默认为[0,1])之间	
		X' = (x-min) / (max-min)	<---对列而言
		X"=X' *(mx -mi)+ mi		<---【mi,mx】

数据归一化API：
	1. sklearn. preprocessing. MinMaxScaler (feature_range=(0, 1)... )
		feature_range 缩放区间
	2.fit_transform(X）将特征进行归一化缩放

案例：演示特征预处理之归一化操作.
	回顾：特征工程的目的和步骤
	目的：
	利用专业的背景知识和技巧处理数据，用于提升模型的性能.
	步骤：
		1．特征提取.
		2.特征预处理（归一化，标准化）
		3.特征降维.
		4.特征选择.
		5.特征组合.

标准化:
	x'=（当前值x－该列平均值x.mean）／该列的标准差 σ

	无论是归一化，还是标准化，目的都是为了解决因为量纲（单位）问题，导致模型评估较低等问题。

方差计算公式σ^2：	该列每个值和该列 均值.mean_ 的差的平方和的平均值.	.var_
标准差计算公式σ：	方差开平方根	.scale_


正态分布是一种概率分布:
	也叫高斯分布，钟形分布。
	正态分布记作N（μ，σ）μ决定了其位置，其标准差σ决定了分布的幅度
	当μ=0，σ=1时的正态分布是标准正态分布

	-------------_1-------0------+1-------------------
				-σ		μ		σ

方差σ^2 == var_ 是在概率论和统计方差衡量一组数据时--离散程度的度量
	其中M为均值n为数据总数

标准差σ 是  方差var  开根号

正态分布的3σ法则（68-95-99.7法则） --_3σ----0----3σ------


---------------利用KNN算法对鸢尾花分类-------------------
实现流程：
	#1获取数据集
	#2数据基本处理
	#3数据集预处理-数据标准化
	#4机器学习（模型训练）
	# 5 模型评估
	#6模型预测


-------------------# 1.导包----------------------
from sklearn.neighbors import KNeighborsClassifier #KNN算法分类对象
from sklearn.preprocessing import StandardScaler #数据标准化的
from sklearn.datasets import load_iris  #加载鸢尾花测试集的.
	# 导入数据集的分割方法 
from sklearn.model_selection import train_test_split   
	# 导入数据集的评估方法
from sklearn.metrics import mean_squared_error
	#模型评估的，计算模型预测的准确率
from sklearn.metrics import accuracy_score

--------------------------------------------------------

 fit_transform：兼具fit和transform的功能，即：训练，转换。
 	该函数适用于：第一次进行标准化的时候使用.  一般用于处理：训练集。


--------------------超参数选择方法--------------------
	交叉验证、网格搜索、手写数字识别案例

	-----1．知道交叉验证是什么？-------
	
	是一种数据集的分割方法，将训练集划分为n份，拿一份做验证集（测试集）、其他n-1份做训练集
	目的就是为了得到更加准确可信的模型评分。
	
原理：将数据集划分为cv=4份
	1.第一次：把第一份数据做验证集，其他数据做训练
	2.第二次：把第二份数据做验证集，其他数据做训练
	3．··．以此类推，总共训练4次，评估4次。
	4.使用训练集+验证集多次评估模型，取平均值做交叉验证为模型得分
	5.若k=5模型得分最好，再使用全部训练集（训练集+验证集）对k=5模型再训练一边，再使用测试集对k=5模型做评估
	
例子: n_neighbors=[1,3,5,6,7] :每个K:都要4次(4折时)), 5*4==20 共要执行 20次
	
	
	-----2.知道网格搜索是什么？-----
KNeighborsClassifier(n_neighbors=3)  中的 n_neighbors : 超参数

	·模型有很多超参数，其能力也存在很大的差异。需要手动产生很多超参数组合，来训练模型
	·每组超参数都采用交叉验证评估，最后选出最优参数组合建立模型。

网格搜索是 模型调参的有力工具。寻找 最优超参数的工具！
	只需要将若干参数传递给网格搜索对象，它自动帮我们完成不同超参数的组合、模型训练、模型评估，最终返回一组最优的超参数。	

网格搜索+交叉验证的强力组合（模型选择和调优）
	·交义验证解决 模型的数据输入问题（数据集划分）得到更可靠的模型
	·网格搜索解决 超参数的组合
	·两个组合再一起形成一个模型参数调优的解决方案

sklearn.model_selection.GridSearchCV(estimator, param_grid=None,cv=None)
	·对估计器的指定参数值进行详尽搜索
	estimator：估计器对象
	param_grid: 估计器参数(dict)["n_neighbors":[1,3,5]}
	cv：指定几折交叉验证
	
	fit：输入训练数据
	score：准确率

结果分析：
	·bestscore：在交叉验证中验证的最好结果
	bestestimator：最好的参数模型
	cvresults：每次交叉验证后的验证集准确率结果和训练集准确率结果

-------------------------------
交叉验证解释：
	原理：
	把数据分成n份，例如分成：4份   →也叫：4折验证
	第1次：把第1份数据作为验证集(测试集），其它作为训练集，训练模型，模型预测，获取：准确率->准确率1
	第2次：把第2份数据作为验证集（测试集），其它作为训练集，训练模型，模型预测，获取：准确率->准确率2
	第3次：把第3份数据作为验证集（测试集），其它作为训练集，训练模型，模型预测，获取：准确率->准确率3
	第4次：把第4份数据作为验证集（测试集），其它作为训练集，训练模型，模型预测，获取：准确率->准确率4
	然后计算上述的4次准确率的平均值，作为：模型最终的 准确率.
	假设第4次最好（准确率最高），则：用全部数据（训练集+测试集）训练模型，再次用测试集对模型测试
目的：
	为了让模型的最终验真结果更准确

网格搜索：
	目的/作用：
		寻找最优超参数.
	原理：
		接收超参可能出现的值，然后针对于超参的每个值进行交叉验证，获取到最优超参组合。
	超参数：
		需要用户手动录入的数据，不同的超参（组合），可能会影响模型的最终评测结果.

	大白话解释：
		网格搜索+交叉验证，本质上指的是GridSearchCV这个API,
			它会帮我们寻找最优超参（供参考）


3.知道交叉验证网格搜索API函数用法
4.能实践交叉验证网格搜索进行模型超参数调优
5.利用KNN算法实现手写数字识别

