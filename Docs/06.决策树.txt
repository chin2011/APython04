
==============决策树===================
ID3决策树		信息增益

C4.5决策树		信息增益率

CART决策树		GiNi,基尼值

案例泰坦尼克号生存预测
CART回归树
决策树 剪枝

-----------------决策树简介---------------------
决策树是一种树形结构
	树中每个内部节点表示一个特征上的判断，每个分支代表一个判断结果的输出，每个叶子节点代表一种分类结果

决策树的建立过程
	1.特征选择：选取有较强分类能力的特征。
	2.决策树生成：根据选择的特征生成决策树。
	3.决策树也易过拟合，采用剪枝的方法缓解过拟合。

-----------------ID3决策树---------------------
1. 理解信息熵的意义
2. 理解信息增益的作用
3. 知道ID3树的构建流程

信息熵
	·熵 Entropy：信息论中代表随机变量不确定度的度量
	·熵越大，数据的不确定性度越高，信息就越多		←乱
	·熵越小，数据的不确定性越低

（信息）熵 = ∑ - 分类占比 * 1og2（分类占比）

信息增益:
	特征a对训练数据集D的信息增益g(D,a)，定义为集合D的熵H(D|a) 与特征a给定条件下D的熵H（D|a）之差。
	数学公式	g(D, A) = H(D) - H(D|A)
				信息增益  = （信息）熵－条件熵
		D: 所有		A:分类
		
	条件熵=分类占比*特征熵+分类占比*特征熵

信息增益 具体计算过程：
1.计算（信息）熵
	熵=-1/2*1og2（1/2)*2=1
2.计算条件熵
	α的条件熵=
	β的条件熵=
	条件熵=a占比*a的条件熵+ β的占比 * β的条件熵 =
3.计算	信息增益
	信息增益	= 熵 - 条件熵 =


ID3决策树构建流程
	1.计算每个特征的信息增益
	2.使用信息增益 最大的特征将数据集拆分为子集
	3.使用该特征（信息增益最大的特征）作为决策树的一个节点
	4.使用剩余特征对子集重复上述（1，2，3）过程

------------------------------C4.5决策树-----------------------
C4.5决策树		信息增益率

1. 理解信息增益率的意义
2. 知道C4.5树的构建方法

ID3树的不足
	偏向于选择种类多的特征作为分裂依据
	1.特征M作为分裂特征，会构造一棵深度为2的决策树，该树的预测准确率可能非常高
	2.但整棵树过于依赖少数的特征（只根据少数特征进行学习），导致过拟合


信息增益率
	信息增益率 = 信息增益 / 特征熵		← 1/特征熵 = 惩罚系数

	特征熵 = - 特征列 A分类占比*1og2（A分类占比）+
			  - 标签列 B分类占比*1og2（B分类占比）...


信息增益率的本质
	1.特征的信息增益 ÷ 特征的内在信息
	2.相当于对信息增益进行修正，增加一个惩罚系数
	3.特征取值个数较多时，惩罚系数较小；特征取值个数较少时，惩罚系数较大。
	4.惩罚系数：数据集D以 特征a作为随机变量的 熵的 倒数。


结论：特征a的信息增益率大于 > 特征b的信息增益率，
		根据信息增益率，应该选择 特征a 作为分裂特征

-------------------------------------
1信息增益率的作用
	信息增益  偏向于选择种类多的特征作为分裂依据
	缓解ID3树中存在的不足
2信息增益率
	信息增益率 = 信息增益 / 特征熵
	相当于对 信息增益 进行修正，增加一个惩罚系数


-------------------------------CART决策树-------------------------------
CART决策树 (Classification and Regression Tree)
		→ 分类-回归-树 crt
	Car模型是一种决策树模型，它即可以用于 分类，也可以用于 回归。
	Cart回归树使用 平方误差 最小化策略，
	Cart分类生成树采用的基尼指数 最小化策略。
 
基尼值Gini（D）：
	从数据集D中随机抽取两个样本，其类别标记不一致的概率。故，Gini（D）值越小，数据集D的纯度越高。		 越小越好
	1.Gini(D) = 两个样本的概率乘积 = 1-每个类别的 平方 和 = 基尼值
	
	例:10个球 :5红5白,  Gini(D)  ==1 - (5/10)²-0.5² = 0.5

基尼指数Gini-index（D）：
	选择使划分后基尼系数最小的属性作为最优化分属性。
	
	2.各分类占比 * 当前分类的基尼值求和( Gini(D) )

注意：
	1.信息增益（ID3）、信息增益率值越大（C4.5），则说明优先选择该特征。
	2.基尼指数值越小（cart），则说明优先选择该特征。
---------

年收入		(数值时)		60,70,75,......
	1.先将数值型属性升序排列，以相邻中间值作为待确定分裂点：
		60,70 -->65  70,75 -->72.5
	2.以年收入65将样本分为两部分，计算基尼指数
	3.以此类推计算所有分割点的基尼指数，最小的基尼指数为0.3

三分类及以上时:
	分成两分类:   如: 结婚--其它		单身--其它
	
--------------------------泰坦尼克号乘客生存预测-------------------------------
DecisionTreeClassifier(criterion='gini' , max_depth=None, random_state=None)
	Criterion:	特征选择标准
		gini"或"entropy"，前者代表基尼系数，后者代表信息增益。默认"gini"，即CART算法
		
	min_samples_split：内部节点再划分所需最小样本数
	min_samples_1eaf：叶子节点最少样本数
	max_depth：决策树最大深度

	字段:
Passengerld：乘客编号
Survived：生存状态（0代表未存活，1代表存活）
Pclass：舱位等级（1等、2等、3等）
Name：乘客姓名
Sex:性别
Age:年龄
SibSp：在船上的兄弟姐妹或配偶个数
Parch：在船上的父母或孩子个数''
Ticket：船票号码
Fare：票价
Cabin：客舱
Embarked:登船港口 (C=Cherbourg,Q=Queenstown,S=Southampton)

X = data[['Pclass', 'Sex', 'Age']].copy()  # 使用.copy()创建副本避免警告

	# 转换类别值 Sex列: male:0, female:1
X['Sex'] = X['Sex'].map({'male': 0, 'female': 1})
		--->也可以用--->热编码:
		#针对于Sex列，进行one-hot编码。--->会多出一列,要再处理...
		X = pd.get_dummies(X, columns=['Sex'])
	
	
	# 处理Age列的缺失值，用平均年龄填充
X['Age'].fillna(X['Age'].mean(), inplace=True)

------------------------ CART回归决策树------------------------
CART 回归树和 CART 分类树的不同之处在于
	CART分类树预测输出的是一个离散值，
		CART回归树预测输出的是一个连续值
	CART分类树使用基尼指数作为划分、构建树的依据，
		CART回归树使用平方损失
	分类树使用 叶子节点多数类别作为预测类别，
		回归树则采用 叶子节点里均值作为预测输出

CART回归树的平方损失
	Loss(y, f(x)) 	=	(f(x)	 -	y)²
		误差			预测值 	－真实值
₁ ₂ ₃₇ ₈
	分两边,两边都求平均值m₁ ,m₂ ,	<---y_pre	
		(m₁-y₁) ², (m₁-y ₂)², ...
		(m₂-y₇)², (m₂-y₈)², ...		

	3.以此方式计算2.5、3.5...等划分点的平方损失，
	4.当划分点s=6.5时，m（s）最小。所以第1个划分变量：特征为×，切分点为6.5
...
	7,假设在生成3个区域之后停止划分，以上就是最终回归树。
		·每一个叶子节点的输出为：挂在该节点上的所有样本均值。

CART回归树构建过程小结
	1选择一个特征，将该特征的值进行排序，取相邻点计算均值作为待划分点
	2根据所有划分点，将数据集分成两部分：R1、R2
	3 R1和 R2两部分的平方损失相加作为该切分点平方损失
	4 取最小的平方损失的划分点，作为当前特征的划分点
	5以此计算其他特征的最优划分点、以及该划分点对应的损失值
	6在所有的特征的划分点中，选择出最小平方损失的划分点，作为当前树的分裂点


x_train = np.array(list(range(1, 11))).reshape(-1,1)
	reshape(-1,1): 	-1: 有多少行就转多少行. 	1: 一列:


分类回归决策树：CART
	分类：常用
	回归：不常用

---------------------------------决策树剪枝-----------------------------
决策树剪枝
	是一种防止决策树 过拟合的一种正则化方法；提高其泛化能力。

剪枝
	把子树的节点全部删掉，使用用叶子节点来替换

剪枝方法
	1.预剪枝：
		指在决策树生成过程中，对每个节点在划分前先进行估计，若当前节点的划分不能带来决策树泛化性能提升，则停止划分并将当前节点标记为叶节点；
		优点：计算开销小；缺点：可能欠拟合（“贪心”剪枝可能错过更好的结构）
		
	2.后剪枝：
		是先从训练集生成一棵完整的决策树，然后自底向上地对非叶节点进行考察，若将该节点对应的子树替换为叶节点能带来决策树泛化性能提升，则将该子树替换为叶节点。
		优点：通常比预剪枝效果更好；缺点：计算成本高

	------------
什么是泛化性能？
	泛化性能：指模型对未知数据的预测能力。
	如果一个模型在训练集上准确率很高，但在测试集上表现差，说明它泛化能力差，很可能过拟合了训练数据。

决策树泛化性能提升 = 控制模型复杂度+利用集成方法+合理调参+高质量数据


