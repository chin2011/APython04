
==============逻辑回归=================

==============集成学习=================
误差平方和 SSE 的值		越小越好

肘部法
	下降率 突然变缓 时即认为是最佳的k值

总结口诀：
	SC系数 和 CH指数：	越大越好 	[-1,1]]
	
	CH:	、 质心个数	(越小越好)
	
	DBI 和 Inertia：		越小越好 	
	有真实标签时可用 ARI / NMI（越大越好）
	
如需根据具体数据选择最佳聚类数 k，通常的做法是：
	对多个 k 值计算 CH 或 SC
	选取使 CH 最大 或 SC 最大的 k
	
	
	
-----------------------------------------------
集成学习
	是机器学习中的一种思想，它通过多个模型的组合形成一个精度更高的模型，参与组合的模型成为弱学习器（弱学习器）。训练时，使用训练集依次训练出这些弱学习器，对未知的样本进行预测时，使用这些弱学习器联合进行预测。

集成学习分类
	Bagging：随机森林		<---Parallel 并行
	Boosting: Adaboost、GBDT、XGBoost、LightGBM		<---Sequential 串行


Bagging思想
	有放回的抽样（bootstrap抽样）产生不同的训练集，从而训练不同的学习器
	通过平权投票、多数表决的方式决定预测结果
	弱学习器可以并行训练

Boosting思想
	每一个训练器重点关注前一个训练器不足的地方进行训练
	通过加权投票的方式，得出预测结果
	串行的训练方式
	
	加权:
		预测对了：权重下降
		预测错了：权重提升

	随着学习的积累从弱到强
	每新加入一个弱学习器，整体能力就会得到提升
	代表算法： Adaboost， GBDT， XGBoost， LightGBM


----------------------随机森林算法--------------------------
随机森林
	是基于Bagging思想实现的一种 集成学习算法，采用决策树模型作为每一个弱学习器。
	
1.训练：
	1.有放回的产生训练样本
	2.随机挑选  n个特征（ n小于总特征数量）
	
2.预测：平权投票，多数表决输出预测结果


----------------------Adaboost算法------------------------
Adaptive Boosting（自适应-提升)
	基于Boosting思想实现的一种集成学习算法核心思想, 是通过逐步提高那些被前一步分类错误的样本的权重来训练一个 强分类器。

	预测对了：权重降低
	预测错了：权重提升

2.计算 模型权重(权重变化系数)：1/2*np.1og（（1-0.3)/0.3）=0.4236
	新的权重=样本权重 * 模型权重 = 旧的权重 * 权重变化系数


--------------案例AdaBoost实战葡萄酒数据-------------------
		AdaBoost-->决策树（CART）-->二叉树
需求
	已知葡酒数据, 根据数据进行葡萄酒分类

思路分析
	# 1 读数据到内存
	#2特征处理
		# 2-1修改列名
		#2-2Adaboost一般做二分类去掉一类(1,2,3)
		# 2-4 类别转化 (2,3)=>(0,1)
		#2-5划分数据
	#3实例化单决策树实例化Adaboost-由500颗树组成
	#4单决策树训练和评估
	#5AdaBoost训练和评估

AdaBoost算法介绍：
	它属于Boosting思想，即：串行执行，每次使用全部样本，最后加权投票.
	原理：
		1.使用全部样本，通过决策树模型（第1个弱分类器）进行训练，获取结果.
			思路：
				预测正确→权重下降
				预测错误→权重上升
		2.把第1个弱分类器的处理结果，交给第2个弱分类器进行训练，获取结果.
			思路：
				预测正确→权重下降
				预测错误→权重上升
		3.依次类推，串行执行，直至获取最终结果。
---------------------------
fixed acidity       固定酸度
volatile acidity    挥发性酸度
citric acid 柠檬酸
residual sugar  残糖量
chlorides   氯化物
free sulfur dioxide 游离二氧化硫
total sulfur dioxide    总二氧化硫
density 密度
pH  pH值
sulphates   硫酸盐
alcohol 酒精度
quality 质量 (评分)

-----------------
df_wine= pd.read_csv（'./data/wine0501.csv')

	#[1, 2, 3]
	葡萄酒类别有3种。但是决策树只能识别二叉树。
	#2.数据预处理
	#2.1从标签列（CLasslabeL)中，过滤掉 1类别，剩下2，3类别。
df_wine = df_wine[df_wine['class label'] ≠ 1]

	# print(df_wine['Class label'].unique())
	#[2, 3]

	#2.2获取特征列和标签列.
x= df_wine[['Alcohol','Hue']]y = df_wine['class label']
	#酒精和色泽
	#标签列.
	#2.3打印数据.# print(x[:5])# print(y[:5])
	#2.4通过标签编码器，把标签列，转换为数值列.
le = LabelEncoder()
y = le.fit_transform(y)		# [2, 3] → [0, 1]
I

1.Adaptive Boost:自适应-提升
	逐步调整权重->对（下降），错（上升）

2.Adaboost构建过程
	1初始化数据权重，来训练第1个弱学习器。找最小的错误率计算模型权重，再更新模数据权重。
	2根据更新的数据集权重，来训练第2个弱学习器，再找最小的错误率计算模型权重，再更新模数据权重。
	3依次重复第2步，训练n个弱学习器。组合起来进行预测。结果大于0为正类、结果小于0为负类


--------------------残差提升树 / 梯度提升树GBDT-------------------------------
		提升树（Boosting Decision Tree ）	Gradient（梯度）
思想
	通过拟合残差的思想来进行提升
	残差：真实值 －预测值
生活中的例子
	预测某人的年龄为100岁
	第1次预测：对100岁预测，预测成80岁；100－80=20（残差)
	第2次预测：上一轮残差 20岁 作为目标值，预测成16岁；20－16=4（残差）
	第3次预测：上一轮的残差4岁作为目标值，预测成3.2岁；4－3.2=0.8（残差）
	若三次预测的结果串联起来： 80＋16＋3.2=99.2
	通过拟合残差可将多个弱学习器组成一个强学习器，这就是提升树的最朴素思想

	-梯度 = 真实值 - 预测值 == 残差		<---GBDT属于BDT的一种。

梯度提升树（Gradient Boosting Decision Tree）
	梯度提升树不再拟合残差，而是利用梯度下降的近似方法，
	利用损失函数的 负梯度作为 提升树算法中的残差近似值

GBDT拟合的 负梯度就是 残差。
	如果我们的GBDT进行的是分类问题，则损失函数变为logloss，此时拟合的目标值就是该损失函数的负梯度值


负梯度（残差）=真实值－预测值
	...	===>  预测值 = 真实值求和，求平均值


梯度提升树的构建流程
	1初始化弱学习器（目标值的均值作为预测值）
	2迭代构建学习器，每一个学习器拟合上一个学习器的负梯度
	3直到达到指定的学习器个数
	4当输入未知样本时，将所有弱学习器的输出结果组合起来作为强学习器的输出

----------------梯度提升树－案例泰坦尼克号生存预测-----------------

准确率（Accuracy）:
	所有样本中被正确分类的比例。
	类别分布均衡时有效；类别不平衡时不可靠
		（例如99%负样本，模型全猜负也能有99%准确率）。

精确率（Precision）
	预测为正类的样本中有多少是真的正类。衡量“预测的准确性”。
	减少误报（FP）。例如垃圾邮件检测中，不希望把正常邮件误判为垃圾邮件。

召回率（Recall，也叫查全率、敏感度 Sensitivity）
	意义：所有真实正类中，有多少被成功找出来了。衡量“覆盖能力”。
	关注点：减少漏报（FN）。例如疾病筛查中，不能漏掉真正的患者。	

 F1 分数（F1 Score）
	意义：精确率和召回率的调和平均数，综合两者表现。
	为什么用调和平均？ 因为算术平均会掩盖极端值（如一个高一个低），而调和平均对不平衡更敏感。
	适用场景：当需要同时兼顾 Precision 和 Recall，尤其在类别不平衡时,比 Accuracy 更可靠。

3. 四者之间的关系
	Accuracy 是全局指标，但在不平衡数据中可能具有误导性。
	Precision 与 Recall 通常存在权衡（Trade-off）：
	提高 Recall（抓更多正例）往往会导致 Precision 下降（引入更多 FP）。
	提高 Precision（只预测高置信度正例）可能导致 Recall 下降（漏掉一些正例）。
	F1 Score 是 Precision 和 Recall 的平衡指标，适合用于需要兼顾两者的场景。
	在多分类任务中，这些指标可以通过 宏平均（Macro） 或 微平均（Micro） 扩展。


-----------------------XGBoost极限-梯度-提升树--------------------------
XGBoost (Extreme Gradient Boosting)
	极端梯度提升树，集成学习方法的王牌，在数据挖掘比赛中，大部分获胜者用了XGBoost。

1.	构建模型的方法是 最小化训练数据的损失函数：
		训练的模型复杂度较高，易过拟合。
2、	在损失函数中加入 正则化项，
		提高对未知的测试数据的泛化性能。

XGBoost 的改进：
	在传统 GBDT 基础上引入了二阶导数（Hessian）、正则化项、加权分位数草图、缺失值自动处理等技术，提升泛化能力和训练效率。
	正则化项用来 降低模型的复杂度

特性	            说明
高性能	    C++ 实现，支持多线程、缓存优化、外存计算
正则化	    在目标函数中加入 L1（Lasso）和 L2（Ridge）正则项，防止过拟合
处理缺失值	自动学习缺失值的最佳分裂方向
灵活性	    支持自定义目标函数和评估指标
内置交叉验证	可在每轮 boosting 后进行 CV
支持多种任务	回归、分类（二分类/多分类）、排序（Learning to Rank）等
----------------

结论：
	XGBoost（极限梯度提升树，Extheme Gradient Boosting Tree），基于 打分函数的结果 决定是否分枝的
	
步骤：
	1.在损失函数的基础上+正则化项
	2.基于泰勒展开二阶式进行转换，转成近似函数。
	3.把问题从 样本角度一>叶节点角度进行分析。
	4.得到最终结论，打分函数====>
				Gain值 = 拆分前的分 -（拆分后左子树的分+拆分后右子树的分）

	打分函数 Gain值 ---越小越好
--------------
	要计算 第t轮的结果（预测值），则：第t-1轮（上一轮）结果 是已知的。


1.对树中的每个叶子结点尝试进行分裂
2.计算分裂前 - 分裂后的分数：
	1．如果gain > 0，则分裂之后树的损失更小，会考虑此次分裂
	2.如果gain<0，说明分裂后的分数比分裂前的分数大，此时不建议分裂
3.当触发以下条件时停止分裂：
	1.达到最大深度
	2.叶子结点数量低于某个阈值
	3.所有的结点在分裂不能降低损失
	4.等等...
------------

1.损失函数+正则化（控制每个弱学习的复杂度，叶子节点数和树输出的结果）
2.泰勒展开：二阶，使用t-1个弱学习器构成的集成模型损失求解当前t个弱学习的损失
3.角度转换：统一将样本的计算转换树的角度
4判断一个树是否要进行分类		<----------打分函数

---------------------
XGBoost算法API
	·XGB的安装和使用
		·在sklean机器学习库中没有集成xgb。想要使用xgb，需要手工安装
			pip3 install xgboost
		可以在xgb的官网上查看最新版本：https://xgboost.readthedocs.io/en/latest/
	XGB的编码风格
		支持非sklearn方式，也即是自己的风格
		支持sklearn方式，调用方式保持sklearn的形式

功能	原生 XGBoost 参数名	Scikit-learn API 参数名
学习率	eta         				learning_rate
树的最大深度	max_depth		max_depth（相同）
L1 正则化	alpha	 		    reg_alpha
L2 正则化	lambda	  			  reg_lambda
子采样比例（行）	subsample		subsample（相同）
列采样比例（特征）	colsample_bytree	colsample_bytree（相同）
目标函数	objective	通过模型类自动设置（如 XGBClassifier 默认为 'binary:logistic' 或 'multi:softprob'）
评估指标	eval_metric	    	eval_metric（但 sklearn 接口也支持 scoring 在 cross_val_score 中）
早停轮数	early_stopping_rounds 	early_stopping_rounds（作为 .fit() 的参数）


compute_sample_weight()	--->	平衡权重


params = {
        'booster': 'gbtree',    # 使用树模型
        'objective': 'multi:softprob',  # 多类别分类目标函数
        'num_class': len(np.unique(y_train)),   # 自动确定类别数
        'gamma': 0.1,  # 改变默认值以避免过拟合,默认0.3，增加模型的非线性能力,
        'max_depth': 6,  # 添加深度以适应更复杂数据集，默认:6
        'lambda': 2,    # 添加正则化项以防止过拟合,默认1
        'subsample': 0.7,  # 控制每棵树使用的训练数据比例，避免过拟合,默认1
        'colsample_bytree': 0.7,  # 控制每棵树使用的特征比例，避免过拟合,默认1
        'min_child_weight': 3,  # 控制叶子节点的最小权重和，避免过拟合,默认1
        'eta': 0.1,  # 学习率，控制每棵树的贡献，避免过拟合,默认0.3
        'nthread': 4,  # 使用4个线程进行训练,默认使用CPU核数
        'eval_metric': 'mlogloss'   # 评估指标,默认为rmse
    }


XGBoost的新版本默认使用UBJSON格式保存模型，


-----------------------------朴素贝叶斯算法---------------------------------

	-利用概率值进行分类的一种机器学习算法
	
什么是概率？
	一件事情发生的可能性，取值在【0，1】之间。
	比如：抛硬币正面向上的概率、6面骰子抛出5这一面的概率

---------------------概率数学基础复习----------------------
条件概率：
	表示事件A在另外一个事件B已经发生条件下的发生概率，P(A|B)

	朴素: 假定 A 与 B 相互独立

联合概率：
	表示多个条件同时成立的概率，P(AB) = P(A)*P(B|A) = P(B)* P(A|B)

相互独立：
	如果P(A，B) = P(A)P(B)，则称事件A与事件B相互独立

简言之
	条件概率：在去掉部分样本的情况下，计算某些样本的出现的概率，表示为：P(B|A)
	联合概率：多个事件同时发生的概率是多少，表示为：P(AB）=P(B)*P(A|B)

贝叶斯公式
	P(C | W) =	P(W | C) * P(C) / P(W)
		P(C）表示C出现的概率，	一般是 目标值
		P(W|C）表示C条件W出现的概率
		P(W）表示W出现的概率

		1.P（CW）=P（喜欢|程序员，超重）
		2.P(W|C）=P（程序员，超重|喜欢）
		3.P(C）=P(喜欢)
		4.P（W）=P（程序员，超重）

朴素贝叶斯
	在贝叶斯基础上增加：特征条件独立假设，即：特征之间是互为独立的。
	
	此时，联合概率的计算即可简化为：
		1.P（程序员，超重|喜欢）=P（程序员|喜欢）*P（超重|喜欢）
		2.P(程序员，超重）=P(程序员）*P(超重)

拉普拉斯平滑系数
	为了避免概率值为0，我们在分子和分母分别加上一个数值，
	这就是拉普拉斯平滑系数的作用

------------------------朴素贝叶斯情感分类案例-----------------------
	商品评论情感分析
	
pip install jieba		jieba分词器

	切词--->删除 无效词

1获取数据
2数据基本处理
	2-1处理数据y
	2-2加载 停用词------无效词
	2-3 处理数据x  把文档分词
	2-4统计词频矩阵作为句子特征
3准备训练集测试集
4模型训练
	4-1实例化贝叶斯添加拉普拉斯平滑参数
	4-2模型预测
 5 模型评估
---------------------------

#每个评论都会变成长度为37的列表－>0没有这个词，1有这个词.

	#2.9因为就13条数据，我们把前10条当训练集，后三条当测试集。
x_train = x[:10]
y_train = y[:10]
x_test =  x[10:]
y_test =  y[10:]

---------------------SC轮廓系数法（Silhouette Coefficient）-------------------
SC系数	--->	越大越好
	轮廓系数法考虑簇内的内聚程度（Cohesion），簇外的分离程度（Separation）。
	
其计算过程如下:
	对计算每一个样本i到同簇内其他样本的平均距离ai，该值越小，说明簇内的相似程度越大
	计算每一个样本i到最近簇j内的所有样本的平均距离bij，该值越大，说明该样本越不属于其他簇j
	根据下面公式计算该样本的轮廓系数：S = (b-α) / max(a, b)
	计算所有样本的平均轮廓系数
	轮廓系数的范围为：[-1，1]，SC值越大聚类效果越好

	a：样本i到 簇内其他点的距离平均值
		内聚，值越小，内聚越高
	b：样本i到 其他簇间的距离平均值的最小值
		耦合，值越大，耦合度越低


-------------------------CH轮廓系数法（Calinski-Harabasz Index）---------------
CH系数	--->	越大越好
	考虑簇内的内聚程度、簇外的离散程度、质心的个数
		类别内部数据的距离 平方和 越小越好，类别之间的 距离平方和 越大越好。
		聚类的种类数越少越好

	SSW的含义：相当于SSE，簇内距离, 簇内的内聚程度，	越小越好
		SSE（所有簇 所有样本的 误差平方和）
	SSB的含义：簇间距离, 簇与簇之间的分离程度，		越大越好
	
通过图像可观察到n_clusters=4 取到 -->最大值；最佳值4


--------------------------案例顾客数据聚类分析法----------------------
1.能使用聚类算法完成客户案例分析

2.知道怎么求最佳K值


已知：
	客户性别、年龄、年收入、消费指数
需求：
	对客户进行分析，找到业务突破口，寻找黄金客户


# 打印最大 轮廓系数的K值
    print('最大-轮廓系数的K值为：', np.argmax(sc_list) + 2)


'''
    x[:, 0]:年收入,Annual Income (k$), 
    x[:, 1]: 消费指数,Spending Score (1-100)
    c=y_predict 根据聚类预测结果为每个点着色
    s=50 设置点的大小
    cmap='viridis' 使用viridis颜色映射方案
'''
    plt.scatter(x[:, 0], x[:, 1], c=y_predict, s=50, cmap='viridis')


===============南方电网电力负荷预测===============

时间序列预测简介
电力负荷预测案例介绍
电力负荷预测案例模型开发
电力负荷预测案例改进方向

----------------------------时间序列预测－ 概念------------------------

时间序列预测
	是一种根据历史时间序列数据来预测未来值的方法
任务比较好理解，但预测未来并非易事。


能源:
	电力负荷预测
	风力发电功率预测、光伏发电功率预测
	工业能源消费预测、居民能源消费预测
	电力市场价格预测、能源商品交易量预测

交通:
	交通流量预测
	行驶速度预测
	交通运行指数预测

金融:
	股票价格预测、
	汇率预测、利率预测
	企业资金流量预测、金融机构资金流动性预测
	GDP预测、CPI预测


2.时序预测任务场景分类
	单变量单步：直接建模
	单变量多步：建模+滚动预测、多步分别建模
	多变量单步：直接建模
	多变量多步：建模+滚动预测、多步分别建模、深度学习直接建模

----------------------时序预测算法选择----------------------------
基于统计学的经典时序预测算法：
	简单平均、指数平滑、AR、MA、ARIMA..．
深度学习算法：
	RNN类算法、基于大模型的iTransformer.．·
传统机器学习算法：
	回归模型均可，如线性回归、决策树、随机森林、XGBoost、LightGBM、SVM.．·
	
	不论是传统机器学习算法还是深度学习算法，解决方案主要是将时序数据处理成二维的结构化数据集，训练机器学习模型进行预测。

简单平均：历史数据的平均值--->预测值。


----------------------电力负荷预测-1.模型基础架构搭建---------------------
项目说明：
	该项目基于历史的电力负荷数据，训练XGBoost模型，实现多变量单步的电力负荷预测文件说明：
	data：数据
	log：日志
	model：保存的模型文件
	src：项目主要的业务逻辑，包括机器学习建模相关的代码
	utils：项目中自定义的工具包

-------------------电力负荷预测-2.开发日志工具类(utils/log.py)------------
非核心代码，建议直接拷贝到项目中

1 导包 logging
2定义日志工具类
	2-1指定日志级别关系映射
	2-2 定义_init_方法，配置日志保存路径、logger名称、初始格式、声明logger对象、设置日志级别
	2-3定义get_logger方法，指定生成的日志文件名、在logger对象中添加FileHandler















