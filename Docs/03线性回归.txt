
上标 2（²）、上标 3（³）、上标 1（¹）
上标数字与符号：
W⁰ ¹ ² ³ ⁴ ⁵ ⁶ ⁷ ⁸ ⁹ ⁺ ⁻ ⁼ ⁽ ⁾		eˣ, e², e⁻ᵗ
e²　e³　e⁻¹　eˣ　eʸ　eᵃ　eᵇ　eᶜ
e⁽ˣ⁾　e⁽ˣ⁺¹⁾　e⁽⁻ˣ⁾　e^(x)　e^(x+1)
	---------------------
下标:
W₀ ₁ ₂ ₃ ₄ ₅ ₆ ₇ ₈ ₉


==============线性回归 Linear Regressor=================
线性回归简介
	定义、线性回归的分类、应用场景
线性回归问题的求解
	线性回归API、损失函数、导数和矩阵、正规方程法、梯度下降算法
回归模型评估方法
	MAE、MSE、RMSE
线性回归API和案例
	线性回归API、案例波士顿房价预测
欠拟合与过拟合
	出现原因、解决方法、L1正则化、L2正则化
	
----------------------------------------------

MAE:	Mean Absolute Error（平均-绝对-误差）
	计算预测值与真实值之间绝对误差的平均值，对异常值鲁棒性强（无平方放大效应）。
		（每个样本点）误差的 绝对值 和/样本总数
	
	from sklearn.metrics import mean_absolute_error
	mae = mean_absolute_error(y_true, y_pred)  # y_true为真实值，y_pred为预测值
	
MSE:	Mean Squared Error（均方误差）
	计算预测值与真实值之间误差的 平方 的平均值，对异常值敏感（平方项会放大偏差较大的样本影响），便于求导（适合梯度下降优化）。
		（每个样本点）误差的 平方 和／样本总数
	
	from sklearn.metrics import mean_squared_error
	mse = mean_squared_error(y_true, y_pred)
	
RMSE:	Root Mean Squared Error（均方根误差）
	MSE 的平方根，作用是将误差还原到与原始数据相同的量级（更易解释实际偏差大小），同样对异常值敏感。
	
	import numpy as np
	rmse = np.sqrt(mean_squared_error(y_true, y_pred))

适用场景对比：
	需避免异常值干扰（如常规业务数据预测）→ 优先 MAE；
	关注大偏差样本（如异常检测、风险控制）→ 优先 MSE/RMSE；
	需直观解释误差大小（如预测销售额、流量）→ 优先 RMSE。

--------------------------线性回归简介---------------------------

	利用回归方程（函数）对 一个或多个自变量(特征值)和因变量(目标值)之间关系进行建模的一种分析方式。
	
	一元线性回归:y=kx+b=> k: weight（权重）b: bias（偏置)
			--->   y=wx+b
		目标值只与一个因变量有关系
		
	多元线性回归：y= w1x1 + w2x2 + w3x3 +...  + b
			w=[w1, w2, w3,..]	X=[x1, x2, x3,...]
			---->  y= wT X  + b 
		目标值与多个因变量有关系

	1导入	线性回归包
	2 准备	数据
	3实例化	线性回归模型
	4 训练	线性回归模型
	5模型	预测
    
   		 # 3实例化
    estimator = LinearRegression()
   		 # 4 训练
    estimator.fit(x, y)
   		 #打印模型参数  coef_: 斜率 W     intercept_: 截距 b
    print('模型参数：', estimator.coef_, estimator.intercept_)


误差概念：用预测值y－真实值y就是	误差
损失函数：衡量每个样本预测值与真实值效果的函数，也叫 代价函数、成本函数、目标函数

如何让损失 最小:
    思路1：正规方程法.
    		求导	求偏导	矩阵运算

    思路2：梯度下降法.    
    		1.给定初始位置，步长（学习率）
		2.计算该点当前的梯度的负方向	→负导数的方向: 负
		3.向该负梯度方向移动步长
		4.重复2～3步骤，直至结束

		全梯度下降FGD
		随机梯度下降SGD
		小批量梯度下降mini-batch
		随机平均梯度下降算法

    
误差=预测值一真实值
损失函数=用来描述每个样本和其预测值之间关系的.
		 =各个样本的误差和，越小越好

-----------思路：最小二乘--所有误差平方和-------------
损失函数J(w，b)

Loss（k,b)= (174k + b - 68.5)²+ (180k + b - 75)² + ...
		此时无法计算了，我们先假设b的值=一100，之后我们会用导数的思维再做一次。
	=	(174k - 168.5)²+ (180k  - 175)²   + ....
	=	145416 k² - 281671.6 k + 136496.32
		让损失函数最小，就是让这个式子（值）最小，求导.

	求导=> 2*145416k-281671.6+0 = 0	=> 	k= 0.9685
	代入 y=kx + b ==> 算出预测值:	y_pre= 70.456
---------------------------------------------

想求一条直线更好的拟合所有点y=kx + b
	==>引入损失函数(衡量预测值和真实值效果）Loss(k，b)
	==>通过一个优化方法，求损失函数最小值，得到 K 最优解


----------------基础数学-----------------------------

标量scalar：一个独立存在的数，只有大小没有方向
		a=101
向量vector：向量指一列顺序排列的元素。默认是列向量 ----向量有大小和方向
		Pandas中的Series对象  
		
矩阵matrix：二维数组	Pandas中的 DataFrame

张量Tensor：数组，张量是基于向量和矩阵的推广	 Numpy->ndarray->N维度数组
		2个3*4矩阵		3个2*4 或 4个2*3
	reshape(2,3, 4): 2个3行4列的矩阵

----------------------导数，导函数值----------------------
	微商
	物理：加速度
	几何：斜率．..

导数 Derivative
	函数上某一个点求切线就是导数。瞬时速度变化率
	
	常数、指数函数、幂指数、正弦余弦都有自己的导函数，会查表应用
	特别注意复合函数求导：先对外函数求导，在对内函数求导
	
	复合函数求导：g（h）是外函数h,（x）是内函数。先对外函数求导，再对内函数求导
		{g[h(x)]}'=g'(h)·h'(x)
	y =（x²+2x)²	y'=2(x²+2x) * (x²+2x)' = 2(x²+2x) * (2x +2) ......
		
		(X / Y)' = (X' * Y - X * Y') / Y²
		
		(a^x)' =a^x * lna	---指数函数导数

	导数求极值:	导数为0的位置是函数的极值点
	
------------偏导------------
U是关于x、y、z的函数，记为U(x，y，z)，只在x分量上求导，则为求偏导。

	Z是关于x和y的函数记成z(x,y)，求解 z=（x－2)²+(y－3)² 的极小值

	Partial / Round Z
	-------------------	=（x－2)²' = 2 * (x-2) *(x-2)' =0  ==> x=2
	Partial x
					x=2时可以在x方向求导极小值    Z 对 x 的偏导
					

	Partial  Z
	------------	=( y－3)² ' = 0  	 ==> y=3
	Partial y
					y=3时可以在y方向求导极小值

--------------------------向量和矩阵－向量运算--------------------
向量是有大小和方向
	几何意义上表示：向量（1，1），向量（1，2）

向量矩阵转置Transpose		x^T

向量和矩阵－范数Norm
	范数（norm)是数学中的一种基本概念，具有长度的意义
	1范数（L1范数）-向量中各个元素 绝对值之和
		x^T =(1,2,-3)	||x||₁ = |1| + |2| + |1-3  = 6
		
	2范数（L2范数）-向量的模长，每个元素 平方求和，再开平方根
			||1x||₂  =  ²√ 1² + 2²  + (-3)² = √14
			
			x^T * x = 1² + 2² + (-3)² = 14  ==	||1x||₂² 
	
	p-范数：向量中每一个元素p幂求和，在开p次根  p=1,2,3.....

----------			
矩阵加法和减法		A + - B=C

矩阵乘法：对应行列元素相乘，然后再加和再一起  A@B=C  

矩阵转置	A^T

方阵：一种特殊的矩阵，其行数=列数
	A @ A^T是方阵，A^T @ A 是方阵

	对称方阵：一种特殊的 方阵，沿着主对角线，其元素对称aij= aji

	单位阵：一种特殊的 方阵符号E 或者I 主对角线为1，其他为0

-----------
矩阵乘法的性质
	矩阵的乘法不满足交换律：AXB ≠ BXA
	
	矩阵的乘法满足结合律。即：A×（B×C）=（A×B）×C
	
	矩阵与单位矩阵相乘等于矩阵本身	A @ I  =A	I @ A  =A
	
	矩阵的逆	A@B=I 单位矩阵 则B为A的逆矩阵，记为：A-¹  = B

--------------
矩阵转置的性质
	(A^T) ^T	= A			(A + B)^T = A^T + B^T
	
	(kA)^T =kA^T			(A @ B)^T = B^T @ A^T

----------------------------正规方程法求解----------------------

一元线性回归损失函数
	J（k，b) 	= (预测值1 - 真实值1)² +  (预测值2 - 真实值2)² ...  的极小值
				=(kx1 + b - y1)²  + .......
	1. 	k的偏导:
			= 2 * (kx1 + b - y1) * x1 + ...   ==0		←(x1,y1 都是已知的,代入)
	2.	b的偏导:
			= 2 * (kx1 + b - y1) * 1  + ...   ==0
	====> k=...	 b=...


多元线性回归
	y = w1x1 + w2x2 + w3x3 + ... + b = w^T x+ b
		其中模型权重w是一个向量 w={ w1,w2,......... }
损失函数:

		X = 特征列矩阵   <---不含 y
	
		x₁ w1	x₂ w2	x3 w3	...		y
	1	10		20		30				81
	2	12		25		...				102
	...
	-----------------------------------------------------
		===>	x12 = 20	x22 =25	
	
	第1个样本的预测值：Y1=w1 x11 + w2 x12 +...  + b   	→ Y1 : 预测值
	第1个样本的损失:	ε₁²  =  (Y1 - y1)² 		→  y1:真实值 	ε²:误差
		....
	ε₁²  , ε₂² ,........
	
n个样本样本损失最小：	Loss (W) =  ε₁²  + ε₂²  + .....
	相当于把 第1个样本损失+第2个样本损失+...第n个样本的损失

	只要让多元线性回归损失函数取最小值，此时的权重W（w就是一个向量）就是最优解！
求最优解的方法：
	1解矩阵方程（也就是正规方程）
	2通过梯度下降的方法求解

	J (W) =  ε₁²  + ε₂²  + .....	 = || Xw -y ||₂²		←	x^T * x  ==	||x||₂² 

	对w求导:	J (W)' =  2* || Xw -y ||₂ * 	|| X||  =0
			2 *(Xw- y) * X =0   
			....
			Xw = y
			X^T * Xw = X^T * y	
			...
			w =...

正规方程法存在的问题：
	1.如果运算量过大，可能造成内存溢出
	2.假设矩阵没有逆，则可能无解。
	
	
----------------------------梯度下降和梯度---------------------

顾名思义：沿着梯度下降的方向求解极小值
举个例子：坡度最陡下山法
	梯度下降过程就和下山场景类似
	可微分的损失函数，代表着一座山
	寻找的函数的最小值，也就是山底


输入：初始化位置S；每步距离为a。输出：从位置S到达山底
步骤1：令初始化位置为山的任意位置S
步骤2：在当前位置环顾四周，如果四周都比S高返回S；否则执行步骤3
步骤3：在当前位置环顾四周，寻找  坡度最陡的方向，令其为x方向
步骤4：沿着x方向往下走，长度为a，到达新的位置S'
步骤5：在S'位置环顾四周，如果四周都比S'高，则返回S'。否则转到步骤3

什么是梯度gradient	grad
	单变量函数中，梯度就是某一点 切线斜率（某一点的导数）；有方向为函数增长最快的方向
	多变量函数中，梯度就是某一个点的偏导数；有方向：偏导数分量的向量方向
-----------------

下个点 = 当前点 -  学习率 * 损失函数（某一点的导数）
	α：学习率（步长）,不能太大，也不能太小．机器学习中：0.001～0.01
	梯度是上升最快的方向，我们需要是下降最快的方向，所以需要加负号

------------------------------------
梯度下降优化过程
	1.给定初始位置、步长（学习率）
	2.计算该点当前的梯度的负方向
	3.向该负方向移动步长
	4.重复2-3步直至收敛
		·两次差距小于指定的值
		·达到指定的迭代次数

--------------------------------------------------
有关 学习率 步长(Learning rate)
	1.步长决定了在梯度下降迭代的过程中，每一步沿梯度负方向前进的长度
	2.学习率太小，下降的速度会慢
	3.学习率太大：容易造成错过最低点、产生下降过程中的震荡、甚至梯度爆炸

θ
梯度下降公式，下个点=当前点一学习率*损失函数

回顾：损失函数分类
	最小二乘：∑(预测值 - 真实值) ^2
	均方误差：	MSE
	均方根误差：RMSE
	平均绝对误差：MAE

---------------------------梯度下降法分类-------------------

全梯度下降算法 FGD(Full Gradient Descent)	FGD
	每次迭代时，使用全部样本的梯度值
	→ 由于使用全部数据集，训练速度较慢
	
随机梯度下降算法 SGD
	每次迭代时，随机选择并使用一个样本梯度值
	
	→简单，高效，不稳定。SG每次只使用一个样本迭代，若遇上噪声则容易陷入局部最优解
		---------------------------------
	
小批量梯度下降算法mini-batch
	每次迭代时，随机选择并使用小批量的样本梯度值从m个样本中，选择x个样本进行迭代（1<x<m），
	
	→结合了 SG 的胆大和 FG 的心细，它的表现也正好居于 SG 和 FG 二者之间。
		目前使用最多，正是因为它避开了FG运算效率低成本大和SG收敛效果不稳定的缺点
		
		---------------------------------
随机平均梯度下降算法 SAG
	每次选代时，随机选择一个样本的梯度值和以往样本的梯度值的均值

	1.随机选择一个样本，假设选择D样本，计算其梯度值并存储到列表：
		[D]，然后使用列表中的梯度值均值，更新模型参数。
	2.随机再选择一个样本，假设选择G样本，计算其梯度值并存储到列表：
		[D，G]，然后使用列表中的梯度值均值，更新模型参数。
	3.随机再选择一个样本，假设又选择了D样本，重新计算该样本梯度值，
		并更新列表中D样本的梯度值，使用列表中梯度值均值，更新模型参数。
	4．·..以此类推，直到算法收敛。
	
	→训练初期表现不佳，优化速度较慢。这是因为我们常将初始梯度设为0，而SAG每轮梯度更新都结合了上一轮梯度值。
		------------------------------

	目前使用较多的是：小批量梯度下降
	
-----------------梯度下降法与正规方程对比--------------------------
梯度下降			--->普适
	需要选择学习率
	需要迭代求解
	特征数量较大可以使用
	应用场景：更加普适，迭代的计算方式，适合于嘈杂、大数据应用场景
注意：梯度下降在各种损失函数（目标函数）求解中大量使用。深度学习中更是如此，深度学习模型参数很轻松就上亿，只能通过迭代的方式求最优解。	
	

正规方程			--->小数据,精准
	不需要学习率
	一次运算得出，一蹴而就
	应用场景：小数据量场景、精准的数据场景
	缺点：计算量大、容易收到噪声、特征强相关性的影响
注意：X^T X的逆矩阵不存在时，无法求解
注意：计算X^T X的逆矩阵非常耗时, 如果数据规律不是线性的，无法使用或效果不好

